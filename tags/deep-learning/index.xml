<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Abhishek Nayak</title>
    <link>https://nykabhishek.github.io/tags/deep-learning/</link>
      <atom:link href="https://nykabhishek.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>&amp;copy 2022</copyright><lastBuildDate>Tue, 01 Jan 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Deep Learning</title>
      <link>https://nykabhishek.github.io/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>Reference Machine Vision for ADAS functions</title>
      <link>https://nykabhishek.github.io/project/rmv/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://nykabhishek.github.io/project/rmv/</guid>
      <description>&lt;p&gt;
    &lt;b&gt;Location:&lt;/b&gt; Texas A&amp;M Transportation Institute (TTI) - College Station, TX
&lt;/p&gt;
&lt;h4&gt; &lt;b&gt;Videos:&lt;/b&gt; &lt;/h4&gt;
&lt;p&gt;
&lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/lPJ7cJMTVgI&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/rZTTLVrJFtM&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/U4w29nLmVa0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align:justify;&#34;&gt;
    &lt;b&gt;Abstract:&lt;/b&gt;
    Studies have shown that fatalities due to unintentional roadway departures can be significantly reduced if Lane Departure Warning (LDW) and Lane Keep Assist (LKA) systems are used effectively. However, these systems are not yet popular because the systems are not robust due, in part to the lack of suitable standards for pavement markings that enable reliable functionality of the sensor system. 
    &lt;br&gt;
    The objective of this project is to develop a reference Lane Detection (LD) system that will provide a benchmark for evaluating different lane markings, sensors, and perception algorithms. The goal of the project is to create a system that will validate the effectiveness of lane markings as well as the vision algorithms through a systematic development of LD metrics, and testing procedures for LD algorithms.
&lt;/p&gt;
&lt;p&gt; 
    Follow this &lt;a href=&#34;https://www.vtti.vt.edu/utc/safe-d/index.php/projects/reference-machine-vision-for-adas-functions/&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view the project on Safe-D Website.
&lt;/p&gt;
&lt;p&gt;
    This is Safe-D UTC sponsored project. &lt;a href=&#34;https://rip.trb.org/view/1599232/&#34; target=&#34;_blank&#34;&gt;(https://rip.trb.org/view/1599232)&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;b&gt;Referances:&lt;/b&gt;
    &lt;ol start=&#34;1&#34;&gt;
        &lt;li&gt;Nayak, A., Pike, A., &amp; Rathinam, S.(2022). Effect of pavement markings on machine vision used in ADAS functions (No. 2022-01-0154). SAE Technical Paper.&lt;/li&gt;
        &lt;li&gt;Nayak, A., Pike, A., Rathinam, S., &amp; Gopalswamy, S. (2020). Reference test system for machine vision used for ADAS functions (No. 2020-01-0096). SAE Technical Paper.&lt;/li&gt;
    &lt;/ol&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Response of Autonomous Vehicles to Emergency Response Vehicles (RAVEV)</title>
      <link>https://nykabhishek.github.io/project/ravev/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://nykabhishek.github.io/project/ravev/</guid>
      <description>&lt;!-- &lt;p&gt;
    &lt;b&gt;Location:&lt;/b&gt; Texas A&amp;M Transportation Institute (TTI) - College Station, TX
&lt;/p&gt; --&gt;
&lt;h4&gt; &lt;b&gt;Videos:&lt;/b&gt; &lt;/h4&gt;
&lt;p style=&#34;text-align:center; line-height:80%&#34; &gt; 
    &lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/ILjxfTvve_M&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;small&gt; &lt;b&gt;Emergency Vehicle (EV) detection:&lt;/b&gt; This video shows the emergency vehicles being detected by the trained YOLO-v3 object detector.&lt;/small&gt;&lt;br/&gt;
&lt;br&gt;
    &lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/jC9_wJNKgvk&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;small&gt; &lt;b&gt; RAVEV - Hidden line of sight:&lt;/b&gt; This video shows the response by an autonomous vehicle when being followed by an EV with an obscured line of sight. This video was collected at RELLIS campus facility of Texas A&amp;M University. &lt;/small&gt;&lt;br/&gt;
&lt;br&gt;
    &lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/7__n65RxJSA&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;small&gt; &lt;b&gt; RAVEV - Clear line of sight:&lt;/b&gt; This video shows the response by an autonomous vehicle when being followed by an EV with a clear line of sight. This video was collected at RELLIS campus facility of Texas A&amp;M University. &lt;/small&gt;&lt;br/&gt;
&lt;br&gt;
    &lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/RnoCnT-sKZ0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;small&gt; &lt;b&gt; High-speed lane change:&lt;/b&gt; This video shows in action the high speed vehicle controller developed as a part of the RAVEV project. This video was collected at RELLIS campus facility of Texas A&amp;M University. &lt;/small&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align:center&#34;&gt;
    &lt;h4&gt;&lt;b&gt;&lt;a href=&#34;https://sites.google.com/tamu.edu/ravev/&#34; target=&#34;_blank&#34;&gt;Visit Project Website here&lt;/a&gt;&lt;/b&gt;&lt;/h4&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;h4&gt;Abstract:&lt;/h4&gt;
    The objective of this project is to explore an ideal response action of an autonomous vehicle towards response vehicles in emergency scenarios using vision, sound and other sensors. I developed vision-based algorithms to reliably detect and track emergency vehicles from a video feed using image processing, machine learning, deep neural networks and other computer vison techniques.
&lt;/p&gt;
&lt;p&gt;
    This was a Safe-D UTC sponsored project &lt;a href=&#34;https://rip.trb.org/view/1500797/&#34; target=&#34;_blank&#34;&gt;(https://rip.trb.org/view/1500797)&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt; 
    Follow this &lt;a href=&#34;https://www.vtti.vt.edu/utc/safe-d/index.php/projects/response-of-autonomous-vehicles-to-emergency-response-vehicles/&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view the Safe-D project website.
&lt;/p&gt;
&lt;p&gt;
    The final SAFE-D report can be viewed &lt;a href=&#34;https://safed.vtti.vt.edu/wp-content/uploads/2020/08/03-051_FinalResearchReport_Final.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
    &lt;h4&gt;References:&lt;/h4&gt;
    &lt;ul&gt;
        &lt;li&gt;Nayak, A., Gopalswamy, S., &amp; Rathinam, S. (2019). Vision-Based Techniques for Identifying Emergency Vehicles (No. 2019-01-0889). SAE Technical Paper.&lt;/li&gt;
        &lt;li&gt;Nayak, A., Rathinam, S., &amp; Gopalswamy, S. (2020). Response of Autonomous Vehicles to Emergency Response Vehicles (RAVEV) (No. 03-051).&lt;/li&gt;
    &lt;/ul&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Infrastructure Enabled Autonomy (IEA)</title>
      <link>https://nykabhishek.github.io/project/iea/</link>
      <pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://nykabhishek.github.io/project/iea/</guid>
      <description>&lt;p&gt;
    &lt;b&gt;Location:&lt;/b&gt; &lt;a href=&#34;https://rellis.tamus.edu/&#34; target=&#34;_blank&#34;&gt;The Rellis campus&lt;/a&gt;, Texas A&amp;M University - College Station, TX
    &lt;b&gt;Collaborators:&lt;/b&gt; &lt;a href=&#34;https://cast.tamu.edu/&#34; target=&#34;_blank&#34;&gt;CAST Program&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt; &lt;b&gt;Videos:&lt;/b&gt; &lt;/h4&gt;
&lt;p&gt;
    &lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/s4xNCPnUPRg&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/eOoPIvJhj3k&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/iMSxPE9c2QQ&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/7MCkzDjaPPY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;720&#34; height=&#34;405&#34; src=&#34;https://www.youtube.com/embed/X9t4WEsonf0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align:justify;&#34;&gt;
    &lt;b&gt;Abstract:&lt;/b&gt;
    IEA is a new paradigm in Autonomous Vehicle technology that looks at offloading the core computational capabilities of awareness generation and path planning from the vehicle out onto Smart Roadside Units equipped with various sensors. Through this distributed setup IEA provides a solution of shared liabilities by transferring the primary responsibility of localization from vehicle to infrastructure which in-turn enables of greater situational awareness of the area under the purview of IEA. IEA architecture is deployed on specific sections of roads or special traffic corridors by installing Road-Side Units (RSU) on either side of the road. These RSUs are fitted with multi-sensor smart packs (MSSP) containing sensors required for localizing vehicles. These MSSPs monitor the vehicles in the section of the roads under the purview of IEA and aid in generating the situational awareness which can be transmitted to the vehicles subscribing to this information. 
    &lt;br&gt;
    &lt;br&gt;
    MSSP includes several sensors that carry-out specific individual tasks and as a whole aid in generating the localization information. For example, cameras installed on the RSUs as a part of the MSSP are used to monitor traffic by identifying and locating all the objects of interest in the traffic corridor. MSSPs are installed with special SmartConnect devices, whose function is to establish wireless connectivity between MSSPs and the vehicles subscribing to its information and thus enabling transmission of information necessary for its localization. The SmartConnect devices are communication medium agnostic and modular so that they can be easily substituted by newer technologies.
&lt;/p&gt;
&lt;p&gt;
    &lt;b&gt;Referances:&lt;/b&gt;
    &lt;ol start=&#34;1&#34;&gt;
        &lt;li&gt;Nayak, A., Chour, K., Marr, T., Ravipati, D., Dey, S., Gautam, A., ... &amp; Rathinam, S. (2018). A distributed hybrid hardware-in-the-loop simulation framework for infrastructure enabled autonomy. arXiv preprint arXiv:1802.01787.&lt;/li&gt;
        &lt;li&gt;Ravipati, D., Chour, K., Nayak, A., Marr, T., Dey, S., Gautam, A., ... &amp; Swaminathan, G. (2019, October). Vision Based Localization for Infrastructure Enabled Autonomy. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC) (pp. 1638-1643). IEEE.&lt;/li&gt;
        &lt;li&gt;Gopalswamy, S., &amp; Rathinam, S. (2018, June). Infrastructure enabled autonomy: A distributed intelligence architecture for autonomous vehicles. In 2018 IEEE Intelligent Vehicles Symposium (IV) (pp. 986-992). IEEE.&lt;/li&gt;
    &lt;/ol&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
