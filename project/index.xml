<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Abhishek Nayak</title>
    <link>https://nykabhishek.github.io/project/</link>
      <atom:link href="https://nykabhishek.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>&amp;copy2020</copyright><lastBuildDate>Tue, 01 Jan 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://nykabhishek.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>https://nykabhishek.github.io/project/</link>
    </image>
    
    <item>
      <title>Reference Machine Vision for ADAS functions</title>
      <link>https://nykabhishek.github.io/project/rmv/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://nykabhishek.github.io/project/rmv/</guid>
      <description>&lt;p&gt;
    &lt;b&gt;Location:&lt;/b&gt; Texas A&amp;M Transportation Institute (TTI) - College Station, TX
&lt;/p&gt;
&lt;p&gt;
    &lt;b&gt;Abstract:&lt;/b&gt;
    Studies have shown that fatalities due to unintentional roadway departures can be significantly reduced if Lane Departure Warning (LDW) and Lane Keep Assist (LKA) systems are used effectively. 
    However, these systems are not yet popular because the systems are not robust due, in part to the lack of suitable standards for pavement markings that enable reliable functionality of the sensor system. 
    The objective of this project is to develop a reference Lane Detection (LD) system that will provide a benchmark for evaluating different lane markings, sensors, and perception algorithms. 
    The goal of the project is to create a system that will validate the effectiveness of lane markings as well as the vision algorithms through a systematic development of LD metrics, and testing procedures for LD algorithms.
&lt;/p&gt;
&lt;p&gt; 
    Follow this &lt;a href=&#34;https://www.vtti.vt.edu/utc/safe-d/index.php/projects/reference-machine-vision-for-adas-functions/&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view the project on Safe-D Website.
&lt;/p&gt;
&lt;p&gt;
    This is Safe-D UTC sponsored project. &lt;a href=&#34;https://rip.trb.org/view/1599232/&#34; target=&#34;_blank&#34;&gt;(https://rip.trb.org/view/1599232)&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;b&gt;Referances:&lt;/b&gt;
    &lt;br&gt;Nayak, A., Pike, A., Rathinam, S., &amp; Gopalswamy, S. (2020). Reference test system for machine vision used for ADAS functions (No. 2020-01-0096). SAE Technical Paper.
&lt;/p&gt;
&lt;p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/lPJ7cJMTVgI&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/rZTTLVrJFtM&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/U4w29nLmVa0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Response of Autonomous Vehicles to Emergency Response Vehicles (RAVEV)</title>
      <link>https://nykabhishek.github.io/project/ravev/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://nykabhishek.github.io/project/ravev/</guid>
      <description>&lt;p&gt;
    &lt;b&gt;Location:&lt;/b&gt; Texas A&amp;M Transportation Institute (TTI) - College Station, TX
&lt;/p&gt;
&lt;p&gt;
    &lt;b&gt;Abstract:&lt;/b&gt;
    The objective of this project is to explore an ideal response action of an autonomous vehicle towards response vehicles in emergency scenarios using vision, sound and other sensors. 
    I developed vision-based algorithms to reliably detect and track emergency vehicles from a video feed using image processing, machine learning, deep neural networks and other computer vison techniques. 
&lt;/p&gt;
&lt;p&gt;
    &lt;b&gt;&lt;a href=&#34;https://sites.google.com/tamu.edu/ravev/&#34; target=&#34;_blank&#34;&gt;Project Website&lt;/a&gt;&lt;/b&gt; 
&lt;/p&gt;
&lt;p&gt; 
    Follow this &lt;a href=&#34;https://www.vtti.vt.edu/utc/safe-d/index.php/projects/response-of-autonomous-vehicles-to-emergency-response-vehicles/&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to view the project on Safe-D Website.
&lt;/p&gt;
&lt;p&gt;
    This is a Safe-D UTC sponsored project &lt;a href=&#34;https://rip.trb.org/view/1500797/&#34; target=&#34;_blank&#34;&gt;(https://rip.trb.org/view/1500797)&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
    &lt;b&gt;Referances:&lt;/b&gt;
    &lt;br&gt;Nayak, A., Gopalswamy, S., &amp; Rathinam, S. (2019). Vision-Based Techniques for Identifying Emergency Vehicles (No. 2019-01-0889). SAE Technical Paper.
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/ILjxfTvve_M&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/jC9_wJNKgvk&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/7__n65RxJSA&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Infrastructure Enabled Autonomy (IEA)</title>
      <link>https://nykabhishek.github.io/project/iea/</link>
      <pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://nykabhishek.github.io/project/iea/</guid>
      <description>&lt;p&gt;
    &lt;b&gt;Location:&lt;/b&gt; &lt;a href=&#34;https://cast.tamu.edu/&#34; target=&#34;_blank&#34;&gt;CAST Program&lt;/a&gt;, Texas A&amp;M University - College Station, TX
&lt;/p&gt;
&lt;p&gt;
    &lt;b&gt;Abstract:&lt;/b&gt;
    IEA is a new paradigm in Autonomous Vehicle technology that looks at offloading the core computational capabilities of awareness generation and path planning from the vehicle out onto Smart Roadside Units equipped with various sensors.
    Through this distributed setup IEA provides a solution of shared liabilities by transferring the primary responsibility of localization from vehicle to infrastructure which in-turn enables of greater situational awareness of the area under the purview of IEA.
    IEA architecture is deployed on specific sections of roads or special traffic corridors by installing Road-Side Units (RSU) on either side of the road. 
    These RSUs are fitted with multi-sensor smart packs (MSSP) containing sensors required for localizing vehicles. 
    These MSSPs monitor the vehicles in the section of the roads under the purview of IEA and aid in generating the situational awareness which can be transmitted to the vehicles subscribing to this information. 
    &lt;br&gt;
    &lt;br&gt;
    MSSP includes several sensors that carry-out specific individual tasks and as a whole aid in generating the localization information. 
    For example, cameras installed on the RSUs as a part of the MSSP are used to monitor traffic by identifying and locating all the objects of interest in the traffic corridor. 
    MSSPs are installed with special SmartConnect devices, whose function is to establish wireless connectivity between MSSPs and the vehicles subscribing to its information and thus enabling transmission of information necessary for its localization. 
    The SmartConnect devices are communication medium agnostic and modular so that they can be easily substituted by newer technologies.
&lt;/p&gt;
&lt;p&gt;
    &lt;b&gt;Referances:&lt;/b&gt;
    &lt;ol start=&#34;1&#34;&gt;
        &lt;li&gt;Nayak, A., Chour, K., Marr, T., Ravipati, D., Dey, S., Gautam, A., ... &amp; Rathinam, S. (2018). A distributed hybrid hardware-in-the-loop simulation framework for infrastructure enabled autonomy. arXiv preprint arXiv:1802.01787.&lt;/li&gt;
        &lt;li&gt;Ravipati, D., Chour, K., Nayak, A., Marr, T., Dey, S., Gautam, A., ... &amp; Swaminathan, G. (2019, October). Vision Based Localization for Infrastructure Enabled Autonomy. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC) (pp. 1638-1643). IEEE.&lt;/li&gt;
        &lt;li&gt;Gopalswamy, S., &amp; Rathinam, S. (2018, June). Infrastructure enabled autonomy: A distributed intelligence architecture for autonomous vehicles. In 2018 IEEE Intelligent Vehicles Symposium (IV) (pp. 986-992). IEEE.&lt;/li&gt;
    &lt;/ol&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/s4xNCPnUPRg&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/eOoPIvJhj3k&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/iMSxPE9c2QQ&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/7MCkzDjaPPY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/X9t4WEsonf0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Development of autonomous driving capability on a Ford Focus</title>
      <link>https://nykabhishek.github.io/project/focus/</link>
      <pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://nykabhishek.github.io/project/focus/</guid>
      <description>&lt;p&gt;&lt;b&gt;Location:&lt;/b&gt; CAST Program (&lt;a href=&#34;https://cast.tamu.edu/),&#34;&gt;https://cast.tamu.edu/),&lt;/a&gt; Texas A&amp;amp;M University - College Station, TX&lt;/p&gt;
&lt;!-- &lt;p&gt; --&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;img src=&#39;https://nykabhishek.github.io/images/focus_circuit.jpg&#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- &lt;/p&gt; --&gt;
&lt;p&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/0Qlv_Cc4pwY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/Q8OMI-cLm2E&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
    &lt;b&gt;Collaborators:&lt;/b&gt; Kenny Chour, PhD Student - Texas A&amp;M University
&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
